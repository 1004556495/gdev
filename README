#
# Gdev - Managing GPUs as First-Class Computing Resources
# 
# README
#
# Copyright (C) 2011 Shinpei Kato
# University of California, Santa Cruz
# Systems Research Lab.
# 
# All Rights Reserved.
#

Gdev is an operating system module that manages GPUs as first-class
computing resources. It currently supports only NVIDIA's Fermi
architecuture, but could be easily ported to other artectures, too.

as a background implementation of a GPU device driver. It currently
supports most of NVIDIA's GPU architectures, including Fermi (nvc0). 
Gdev also plans to use Nouveau as another background implementation
given that some Linux distros install Nouveau by default recently.
As Gdev doesn't change any interface provided by PSCNV or Nouveau,
the X window system and its compiz can accelerate on GPUs with Gdev.
However, this requires users to additionally install LIBDRM and DDX.
See https://github.com/pathscale/ for PSCNV and 
http://nouveau.freedesktop.org/wiki/InstallNouveau for Nouveau.
To install Gdev device driver, follow the instructions below.
Let $(TOPDIR) be the top working directory, henceforth.

1. envytools

envytools provides a set of tools that assemble/disassemble firmware
code, GPU code, and macro code. It also provides tools to generate
header files that include GPU command definitions.
Gdev requires envytools to compile fermi firmware code.

cd $(TOPDIR)
git clone git://0x04.net/envytools.git
cd envytools
mkdir build
cd build
cmake .. # may require you to install some packages on your distro
make
sudo make install # will install tools to /usr/local/{bin,lib}

2. Gdev runtime

There are two instances of Gdev runtime that users can choose:
(i) user-space runtime and (ii) kernel-space runtime.
The user-space runtime provides all compute functions in user-space,
while the kernel-space runtime lives in the OS and user-space programs
need to use the ioctl system call to use their functions. The following
assume user-space runtime.

cd $(TOPDIR)/runtime
mkdir build
cd build
../configure # if you want user-space runtime, specify --target=user
make
sudo make install

3. Gdev device driver

cd $(TOPDIR)/driver
mkdir build
cd build
../configure
make
sudo init 3
cd $(TOPDIR)/driver/build
sudo make install
sudo init 5
export LD_LIBRARY_PATH="/usr/local/gdev/lib64:$LD_LIBRARY_PATH"
export PATH="/usr/local/gdev/bin:$PATH"

4. Gdev API test

cd $(TOPDIR)/test/gdev/user/matrixadd
make
./user_test 256 # a[256] + b[256] = c[256]

5. CUDA Driver API

Gdev supports CUDA Driver API. Currently, a set of the supported API functions
is limited, while major functions are implemented. We plan to implement missing
functions, too.
On the other hand, Gdev does not support CUDA Runtime API, for now. 
CUDA Runtime API requires compiler support to implicitly call some internal
functions provided by the runtime library. In order to support CUDA Runtime
API, hence, we need to provide either (i) a compiler that generates internal
functions familiar to Gdev, or (ii) the internal functions that the target
compiler, such as nvcc, generates.  We are trying the latter approach.

cd $(TOPDIR)/cuda/
mkdir build
cd build
../configure # if you want kernel-space CUDA, specify --target=kcuda
make
sudo make install

6. CUDA Driver API test (user-space)

cd $(TOPDIR)/test/cuda/user/matrixadd
make
./user_test 256 # a[256] x b[256] = c[256]
